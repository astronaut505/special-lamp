{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "ticker = \"^GSPC\"\n",
    "start_date = \"1900-01-01\"\n",
    "end_date = \"2015-12-31\"\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "# Calculate the E/P ratio\n",
    "data[\"E_P_ratio\"] = 1 / data[\"Close\"]\n",
    "\n",
    "# Calculate the lagged E/P ratio\n",
    "data[\"E_P_ratio_lagged\"] = data[\"E_P_ratio\"].shift(1)\n",
    "\n",
    "# Calculate the returns\n",
    "data[\"Returns\"] = data[\"Close\"].pct_change()\n",
    "data[\"Returns_lagged\"] = data[\"Returns\"].shift(1)\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(data[\"E_P_ratio_lagged\"])\n",
    "y = data[\"Returns_lagged\"]\n",
    "\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5243bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stock_weight(cape, cape_values, percentile_5, percentile_95):\n",
    "    # Calculate E/P (1/CAPE)\n",
    "    ep = 1 / cape\n",
    "    \n",
    "    # Trim E/P\n",
    "    trimmed_ep = np.clip(ep, percentile_5, percentile_95)\n",
    "    \n",
    "    # Calculate stock weight\n",
    "    stock_weight = 100 + (trimmed_ep - np.median(cape_values)) / (percentile_95 - percentile_5)\n",
    "    \n",
    "    # Impose upper and lower bounds on stock weight\n",
    "    stock_weight = np.clip(stock_weight, 50, 150)\n",
    "    \n",
    "    return stock_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e2f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"E_P_ratio_rolling_median\"] = data[\"E_P_ratio\"].rolling(window=120, min_periods=1).median()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data.index, data[\"E_P_ratio\"], label=\"E/P ratio\")\n",
    "plt.plot(data.index, data[\"E_P_ratio_rolling_median\"], label=\"Rolling median\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"E/P ratio\")\n",
    "plt.title(\"Chart 1: E/P ratio and Rolling Median\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(data.head())\n",
    "print(data[\"E_P_ratio\"].describe())\n",
    "print(data[\"E_P_ratio\"].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e12f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"E_P_ratio_rolling_median\"] = data[\"E_P_ratio\"].rolling(window=120, min_periods=1).median()\n",
    "\n",
    "percentile_95 = data[\"E_P_ratio\"].rolling(window=120, min_periods=1).quantile(0.95)\n",
    "percentile_5 = data[\"E_P_ratio\"].rolling(window=120, min_periods=1).quantile(0.05)\n",
    "data[\"E_P_ratio_percentile_range\"] = percentile_95 - percentile_5\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data.index, data[\"E_P_ratio\"], label=\"E/P ratio\")\n",
    "plt.plot(data.index, data[\"E_P_ratio_rolling_median\"], label=\"Rolling median\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"E/P ratio\")\n",
    "plt.title(\"Chart 1: E/P ratio and Rolling Median\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data.index, data[\"E_P_ratio\"], label=\"E/P ratio\")\n",
    "plt.fill_between(data.index, percentile_5, percentile_95, alpha=0.8, label=\"Percentile Range\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"E/P ratio\")\n",
    "plt.title(\"Chart 2: E/P ratio and Percentile Range\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce576353",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"E_P_ratio_percentile_range\"] = percentile_95 - percentile_5\n",
    "data[\"E_P_ratio\"] = 1 / data[\"Close\"]\n",
    "# Calculate lagged returns\n",
    "data[\"Returns_lagged\"] = data[\"Close\"].pct_change().shift(1)\n",
    "\n",
    "# Drop rows with missing values again\n",
    "data = data.dropna()\n",
    "\n",
    "def calculate_stock_weight(ep, percentile_5, percentile_95, median_ep):\n",
    "    # Trim E/P\n",
    "    trimmed_ep = np.clip(ep, percentile_5, percentile_95)\n",
    "    \n",
    "    # Calculate stock weight\n",
    "    stock_weight = 100 + (trimmed_ep - median_ep) / (percentile_95 - percentile_5)\n",
    "    \n",
    "    # Impose upper and lower bounds on stock weight\n",
    "    stock_weight = np.clip(stock_weight, 50, 150)\n",
    "    \n",
    "    return stock_weight\n",
    "\n",
    "# Calculate the market timing strategy's equity weight\n",
    "data[\"Equity_Weight\"] = calculate_stock_weight(data[\"E_P_ratio\"], percentile_5, percentile_95, np.median(data[\"E_P_ratio\"]))\n",
    "\n",
    "# Limit the equity weight between 50% and 150%\n",
    "data[\"Equity_Weight\"] = np.clip(data[\"Equity_Weight\"], 50, 150)\n",
    "\n",
    "\n",
    "# Calculate the market timing strategy's equity weight\n",
    "#data[\"Equity_Weight\"] = 100 + (data[\"E_P_ratio\"] - data[\"E_P_ratio_rolling_median\"]) / data[\"E_P_ratio_percentile_range\"]\n",
    "\n",
    "# Limit the equity weight between 50% and 150%\n",
    "#data[\"Equity_Weight\"] = np.clip(data[\"Equity_Weight\"], 50, 150)\n",
    "\n",
    "# Calculate the cash weight as the complement of the equity weight\n",
    "data[\"Cash_Weight\"] = 100 - data[\"Equity_Weight\"]\n",
    "\n",
    "# Calculate the market timing strategy's returns\n",
    "data[\"Strategy_Returns\"] = data[\"Returns_lagged\"] * (data[\"Equity_Weight\"] / 100) + data[\"Returns_lagged\"].shift(1) * (data[\"Cash_Weight\"] / 100)\n",
    "\n",
    "# Calculate the cumulative returns of the market timing strategy\n",
    "data[\"Cumulative_Strategy_Returns\"] = (1 + data[\"Strategy_Returns\"]).cumprod()\n",
    "\n",
    "# Calculate the cumulative returns of the underlying broad market index (buy-and-hold strategy)\n",
    "data[\"Cumulative_BuyHold_Returns\"] = (1 + data[\"Returns_lagged\"]).cumprod()\n",
    "\n",
    "# Plot the cumulative returns of the market timing strategy and the buy-and-hold strategy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data.index, data[\"Cumulative_Strategy_Returns\"], label=\"Market Timing Strategy\")\n",
    "plt.plot(data.index, data[\"Cumulative_BuyHold_Returns\"], label=\"Buy-and-Hold Strategy\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cumulative Returns\")\n",
    "plt.title(\"Comparison of Market Timing Strategy and Buy-and-Hold Strategy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(data[\"E_P_ratio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb47ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cumulative returns of the market timing strategy and the buy-and-hold strategy\n",
    "cumulative_strategy_returns = data[\"Cumulative_Strategy_Returns\"].iloc[-1]\n",
    "cumulative_buyhold_returns = data[\"Cumulative_BuyHold_Returns\"].iloc[-1]\n",
    "\n",
    "# Calculate the annualized returns of the market timing strategy and the buy-and-hold strategy\n",
    "strategy_annual_returns = (cumulative_strategy_returns ** (252 / len(data))) - 1\n",
    "buyhold_annual_returns = (cumulative_buyhold_returns ** (252 / len(data))) - 1\n",
    "\n",
    "# Calculate the volatility of the market timing strategy and the buy-and-hold strategy\n",
    "strategy_volatility = data[\"Strategy_Returns\"].std() * np.sqrt(252)\n",
    "buyhold_volatility = data[\"Returns_lagged\"].std() * np.sqrt(252)\n",
    "\n",
    "# Calculate the Sharpe ratio of the market timing strategy and the buy-and-hold strategy\n",
    "risk_free_rate = 0.03  # Adjust the risk-free rate if necessary\n",
    "strategy_sharpe_ratio = (strategy_annual_returns - risk_free_rate) / strategy_volatility\n",
    "buyhold_sharpe_ratio = (buyhold_annual_returns - risk_free_rate) / buyhold_volatility\n",
    "\n",
    "# Print the performance metrics\n",
    "print(\"Market Timing Strategy:\")\n",
    "print(\"Cumulative Returns:\", cumulative_strategy_returns)\n",
    "print(\"Annualized Returns:\", strategy_annual_returns)\n",
    "print(\"Volatility:\", strategy_volatility)\n",
    "print(\"Sharpe Ratio:\", strategy_sharpe_ratio)\n",
    "\n",
    "print(\"\\nBuy-and-Hold Strategy:\")\n",
    "print(\"Cumulative Returns:\", cumulative_buyhold_returns)\n",
    "print(\"Annualized Returns:\", buyhold_annual_returns)\n",
    "print(\"Volatility:\", buyhold_volatility)\n",
    "print(\"Sharpe Ratio:\", buyhold_sharpe_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f515c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate additional statistics\n",
    "strategy_returns = data[\"Strategy_Returns\"]\n",
    "buyhold_returns = data[\"Returns_lagged\"]\n",
    "\n",
    "strategy_mean = strategy_returns.mean()\n",
    "buyhold_mean = buyhold_returns.mean()\n",
    "\n",
    "strategy_std = strategy_returns.std()\n",
    "buyhold_std = buyhold_returns.std()\n",
    "\n",
    "strategy_skewness = strategy_returns.skew()\n",
    "buyhold_skewness = buyhold_returns.skew()\n",
    "\n",
    "strategy_kurtosis = strategy_returns.kurtosis()\n",
    "buyhold_kurtosis = buyhold_returns.kurtosis()\n",
    "\n",
    "# Visualize the distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(strategy_returns, kde=True, color=\"blue\")\n",
    "plt.xlabel(\"Returns\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Market Timing Strategy\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(buyhold_returns, kde=True, color=\"green\")\n",
    "plt.xlabel(\"Returns\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Buy-and-Hold Strategy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the additional statistics\n",
    "print(\"Market Timing Strategy:\")\n",
    "print(\"Mean Returns:\", strategy_mean)\n",
    "print(\"Standard Deviation:\", strategy_std)\n",
    "print(\"Skewness:\", strategy_skewness)\n",
    "print(\"Kurtosis:\", strategy_kurtosis)\n",
    "\n",
    "print(\"\\nBuy-and-Hold Strategy:\")\n",
    "print(\"Mean Returns:\", buyhold_mean)\n",
    "print(\"Standard Deviation:\", buyhold_std)\n",
    "print(\"Skewness:\", buyhold_skewness)\n",
    "print(\"Kurtosis:\", buyhold_kurtosis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9e83941b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting alpha_vantage\n",
      "  Downloading alpha_vantage-2.3.1-py3-none-any.whl (31 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp39-cp39-win_amd64.whl (323 kB)\n",
      "     ------------------------------------ 323.6/323.6 kB 770.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from alpha_vantage) (2.28.1)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-win_amd64.whl (34 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp->alpha_vantage) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp->alpha_vantage) (21.4.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp39-cp39-win_amd64.whl (61 kB)\n",
      "     ---------------------------------------- 61.7/61.7 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->alpha_vantage) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->alpha_vantage) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->alpha_vantage) (1.26.11)\n",
      "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, alpha_vantage\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 alpha_vantage-2.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 yarl-1.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sp500_companies.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6448\\200860836.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"FK67MYMVN217OZUE\"\u001b[0m  \u001b[1;31m# Replace with your actual API key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTimeSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pandas'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msp500_companies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sp500_companies.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mearnings_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sp500_companies.csv'"
     ]
    }
   ],
   "source": [
    "!pip install alpha_vantage\n",
    "import pandas as pd\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "api_key = \"FK67MYMVN217OZUE\"  # Replace with your actual API key\n",
    "ts = TimeSeries(key=api_key, output_format='pandas')\n",
    "sp500_companies = pd.read_csv(\"sp500_companies.csv\")\n",
    "earnings_data = {}\n",
    "\n",
    "for index, row in sp500_companies.iterrows():\n",
    "    symbol = row[\"Symbol\"]\n",
    "    try:\n",
    "        earnings, _ = ts.get_earnings(symbol)\n",
    "        earnings_data[symbol] = earnings\n",
    "    except:\n",
    "        print(f\"Failed to retrieve earnings data for {symbol}\")\n",
    "\n",
    "earnings_df = pd.concat(earnings_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea2d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1257 sha256=ecad4e3f77cad407d6adbc7f4b5728f827bd7f86023f079f8b66aa4b95b7ca28\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\73\\2b\\cb\\099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n",
      "No earnings data available for MMM\n",
      "No earnings data available for AOS\n",
      "No earnings data available for ABT\n",
      "No earnings data available for ABBV\n",
      "No earnings data available for ACN\n",
      "No earnings data available for ATVI\n",
      "No earnings data available for ADM\n",
      "No earnings data available for ADBE\n",
      "No earnings data available for ADP\n",
      "No earnings data available for AAP\n",
      "No earnings data available for AES\n",
      "No earnings data available for AFL\n",
      "No earnings data available for A\n",
      "No earnings data available for APD\n",
      "No earnings data available for AKAM\n",
      "No earnings data available for ALK\n",
      "No earnings data available for ALB\n",
      "No earnings data available for ARE\n",
      "No earnings data available for ALGN\n",
      "No earnings data available for ALLE\n",
      "No earnings data available for LNT\n",
      "No earnings data available for ALL\n",
      "No earnings data available for GOOGL\n",
      "No earnings data available for GOOG\n",
      "No earnings data available for MO\n",
      "No earnings data available for AMZN\n",
      "No earnings data available for AMCR\n",
      "No earnings data available for AMD\n",
      "No earnings data available for AEE\n",
      "No earnings data available for AAL\n",
      "No earnings data available for AEP\n",
      "No earnings data available for AXP\n",
      "No earnings data available for AIG\n",
      "No earnings data available for AMT\n",
      "No earnings data available for AWK\n",
      "No earnings data available for AMP\n",
      "No earnings data available for ABC\n",
      "No earnings data available for AME\n",
      "No earnings data available for AMGN\n",
      "No earnings data available for APH\n",
      "No earnings data available for ADI\n",
      "Failed to retrieve earnings data for ANSS\n",
      "No earnings data available for AON\n",
      "No earnings data available for APA\n",
      "No earnings data available for AAPL\n",
      "No earnings data available for AMAT\n",
      "No earnings data available for APTV\n",
      "No earnings data available for ACGL\n",
      "No earnings data available for ANET\n",
      "No earnings data available for AJG\n",
      "No earnings data available for AIZ\n",
      "No earnings data available for T\n",
      "No earnings data available for ATO\n",
      "No earnings data available for ADSK\n",
      "No earnings data available for AZO\n",
      "No earnings data available for AVB\n",
      "No earnings data available for AVY\n",
      "No earnings data available for AXON\n",
      "No earnings data available for BKR\n",
      "No earnings data available for BALL\n",
      "No earnings data available for BAC\n",
      "No earnings data available for BBWI\n",
      "No earnings data available for BAX\n",
      "No earnings data available for BDX\n",
      "No earnings data available for WRB\n",
      "No earnings data available for BRK.B\n",
      "No earnings data available for BBY\n",
      "No earnings data available for BIO\n",
      "No earnings data available for TECH\n",
      "No earnings data available for BIIB\n",
      "No earnings data available for BLK\n",
      "No earnings data available for BK\n",
      "No earnings data available for BA\n",
      "No earnings data available for BKNG\n",
      "No earnings data available for BWA\n",
      "No earnings data available for BXP\n",
      "No earnings data available for BSX\n",
      "No earnings data available for BMY\n",
      "No earnings data available for AVGO\n",
      "No earnings data available for BR\n",
      "No earnings data available for BRO\n",
      "No earnings data available for BF.B\n",
      "No earnings data available for BG\n",
      "No earnings data available for CHRW\n",
      "No earnings data available for CDNS\n",
      "No earnings data available for CZR\n",
      "No earnings data available for CPT\n",
      "No earnings data available for CPB\n",
      "No earnings data available for COF\n",
      "No earnings data available for CAH\n",
      "No earnings data available for KMX\n",
      "No earnings data available for CCL\n",
      "No earnings data available for CARR\n",
      "No earnings data available for CTLT\n",
      "No earnings data available for CAT\n",
      "No earnings data available for CBOE\n",
      "No earnings data available for CBRE\n",
      "No earnings data available for CDW\n",
      "No earnings data available for CE\n",
      "No earnings data available for CNC\n",
      "No earnings data available for CNP\n",
      "No earnings data available for CDAY\n",
      "No earnings data available for CF\n",
      "No earnings data available for CRL\n",
      "No earnings data available for SCHW\n",
      "No earnings data available for CHTR\n",
      "No earnings data available for CVX\n",
      "No earnings data available for CMG\n",
      "No earnings data available for CB\n",
      "No earnings data available for CHD\n",
      "No earnings data available for CI\n",
      "No earnings data available for CINF\n",
      "No earnings data available for CTAS\n",
      "No earnings data available for CSCO\n",
      "No earnings data available for C\n",
      "No earnings data available for CFG\n",
      "No earnings data available for CLX\n",
      "No earnings data available for CME\n",
      "No earnings data available for CMS\n",
      "No earnings data available for KO\n",
      "No earnings data available for CTSH\n",
      "No earnings data available for CL\n",
      "No earnings data available for CMCSA\n",
      "No earnings data available for CMA\n",
      "No earnings data available for CAG\n",
      "No earnings data available for COP\n",
      "No earnings data available for ED\n",
      "No earnings data available for STZ\n",
      "No earnings data available for CEG\n",
      "No earnings data available for COO\n",
      "No earnings data available for CPRT\n",
      "No earnings data available for GLW\n",
      "No earnings data available for CTVA\n",
      "No earnings data available for CSGP\n",
      "No earnings data available for COST\n",
      "No earnings data available for CTRA\n",
      "No earnings data available for CCI\n",
      "No earnings data available for CSX\n",
      "No earnings data available for CMI\n",
      "No earnings data available for CVS\n",
      "No earnings data available for DHI\n",
      "No earnings data available for DHR\n",
      "No earnings data available for DRI\n",
      "No earnings data available for DVA\n",
      "No earnings data available for DE\n",
      "No earnings data available for DAL\n",
      "No earnings data available for XRAY\n",
      "No earnings data available for DVN\n",
      "No earnings data available for DXCM\n",
      "No earnings data available for FANG\n",
      "No earnings data available for DLR\n",
      "No earnings data available for DFS\n",
      "No earnings data available for DISH\n",
      "No earnings data available for DIS\n",
      "No earnings data available for DG\n",
      "No earnings data available for DLTR\n",
      "No earnings data available for D\n",
      "No earnings data available for DPZ\n",
      "No earnings data available for DOV\n",
      "No earnings data available for DOW\n",
      "No earnings data available for DTE\n",
      "No earnings data available for DUK\n",
      "No earnings data available for DD\n",
      "No earnings data available for DXC\n",
      "No earnings data available for EMN\n",
      "No earnings data available for ETN\n",
      "No earnings data available for EBAY\n",
      "No earnings data available for ECL\n",
      "No earnings data available for EIX\n",
      "No earnings data available for EW\n",
      "No earnings data available for EA\n",
      "No earnings data available for ELV\n",
      "No earnings data available for LLY\n",
      "No earnings data available for EMR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No earnings data available for ENPH\n",
      "No earnings data available for ETR\n",
      "No earnings data available for EOG\n",
      "No earnings data available for EPAM\n",
      "No earnings data available for EQT\n",
      "No earnings data available for EFX\n",
      "No earnings data available for EQIX\n",
      "No earnings data available for EQR\n",
      "No earnings data available for ESS\n",
      "No earnings data available for EL\n",
      "No earnings data available for ETSY\n",
      "No earnings data available for RE\n",
      "No earnings data available for EVRG\n",
      "No earnings data available for ES\n",
      "No earnings data available for EXC\n",
      "No earnings data available for EXPE\n",
      "No earnings data available for EXPD\n",
      "No earnings data available for EXR\n",
      "No earnings data available for XOM\n",
      "No earnings data available for FFIV\n",
      "No earnings data available for FDS\n",
      "No earnings data available for FICO\n",
      "No earnings data available for FAST\n",
      "No earnings data available for FRT\n",
      "No earnings data available for FDX\n",
      "No earnings data available for FITB\n",
      "No earnings data available for FSLR\n",
      "No earnings data available for FE\n",
      "No earnings data available for FIS\n",
      "No earnings data available for FI\n",
      "No earnings data available for FLT\n",
      "No earnings data available for FMC\n",
      "No earnings data available for F\n",
      "No earnings data available for FTNT\n",
      "No earnings data available for FTV\n",
      "No earnings data available for FOXA\n",
      "No earnings data available for FOX\n",
      "No earnings data available for BEN\n",
      "No earnings data available for FCX\n",
      "No earnings data available for GRMN\n",
      "No earnings data available for IT\n",
      "No earnings data available for GEHC\n",
      "No earnings data available for GEN\n",
      "No earnings data available for GNRC\n",
      "No earnings data available for GD\n",
      "No earnings data available for GE\n",
      "No earnings data available for GIS\n",
      "No earnings data available for GM\n",
      "No earnings data available for GPC\n",
      "No earnings data available for GILD\n",
      "No earnings data available for GL\n",
      "No earnings data available for GPN\n",
      "No earnings data available for GS\n",
      "No earnings data available for HAL\n",
      "No earnings data available for HIG\n",
      "No earnings data available for HAS\n",
      "No earnings data available for HCA\n",
      "No earnings data available for PEAK\n",
      "No earnings data available for HSIC\n",
      "No earnings data available for HSY\n",
      "No earnings data available for HES\n",
      "No earnings data available for HPE\n",
      "No earnings data available for HLT\n",
      "No earnings data available for HOLX\n",
      "No earnings data available for HD\n",
      "No earnings data available for HON\n",
      "No earnings data available for HRL\n",
      "No earnings data available for HST\n",
      "No earnings data available for HWM\n",
      "No earnings data available for HPQ\n",
      "No earnings data available for HUM\n",
      "No earnings data available for HBAN\n",
      "No earnings data available for HII\n",
      "No earnings data available for IBM\n",
      "No earnings data available for IEX\n",
      "No earnings data available for IDXX\n",
      "No earnings data available for ITW\n",
      "No earnings data available for ILMN\n",
      "No earnings data available for INCY\n",
      "No earnings data available for IR\n",
      "No earnings data available for PODD\n",
      "No earnings data available for INTC\n",
      "No earnings data available for ICE\n",
      "No earnings data available for IFF\n",
      "No earnings data available for IP\n",
      "No earnings data available for IPG\n",
      "No earnings data available for INTU\n",
      "No earnings data available for ISRG\n",
      "No earnings data available for IVZ\n",
      "No earnings data available for INVH\n",
      "No earnings data available for IQV\n",
      "No earnings data available for IRM\n",
      "No earnings data available for JBHT\n",
      "No earnings data available for JKHY\n",
      "No earnings data available for J\n",
      "No earnings data available for JNJ\n",
      "No earnings data available for JCI\n",
      "No earnings data available for JPM\n",
      "No earnings data available for JNPR\n",
      "No earnings data available for K\n",
      "No earnings data available for KDP\n",
      "No earnings data available for KEY\n",
      "No earnings data available for KEYS\n",
      "No earnings data available for KMB\n",
      "No earnings data available for KIM\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas requests bs4\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "sp500_companies = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "sp500_symbols = sp500_companies['Symbol'].tolist()\n",
    "def scrape_earnings(symbol):\n",
    "    url = f\"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={symbol}&type=10-K\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the table containing the earnings data\n",
    "    table = soup.find(\"table\", class_=\"tableFile2\")\n",
    "\n",
    "    # Extract the relevant information from the table, such as filing date, form type, and document link\n",
    "    earnings = []\n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "        for row in rows[1:]:\n",
    "            cells = row.find_all(\"td\")\n",
    "            filing_date = cells[3].text.strip()\n",
    "            form_type = cells[0].text.strip()\n",
    "            document_link = \"https://www.sec.gov\" + cells[1].find(\"a\")[\"href\"]\n",
    "            earnings.append({\"Filing Date\": filing_date, \"Form Type\": form_type, \"Document Link\": document_link})\n",
    "\n",
    "    return pd.DataFrame(earnings)\n",
    "earnings_data = {}\n",
    "\n",
    "for symbol in sp500_symbols:\n",
    "    try:\n",
    "        earnings = scrape_earnings(symbol)\n",
    "        if not earnings.empty:\n",
    "            earnings_data[symbol] = earnings\n",
    "        else:\n",
    "            print(f\"No earnings data available for {symbol}\")\n",
    "    except:\n",
    "        print(f\"Failed to retrieve earnings data for {symbol}\")\n",
    "\n",
    "if earnings_data:\n",
    "    earnings_df = pd.concat(earnings_data)\n",
    "else:\n",
    "    print(\"No earnings data available for any S&P 500 company.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd6068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
